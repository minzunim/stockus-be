from oauth2client.service_account import ServiceAccountCredentials
import gspread
import time
from datetime import datetime, timezone, timedelta
import json
import requests
import random 
import os
from dotenv import load_dotenv

from openai import OpenAI

from app.services.reddit import RedditService
from konlpy.tag import Okt  # ë˜ëŠ” Mecab
from sklearn.feature_extraction.text import TfidfVectorizer

load_dotenv()

# êµ¬ê¸€ ìŠ¤í”„ë ˆë“œ ì‹œíŠ¸ì— ì €ì¥
scope = [
    "https://spreadsheets.google.com/feeds",
    "https://www.googleapis.com/auth/drive"
    ]

creds = ServiceAccountCredentials.from_json_keyfile_name("stock-project-456213-00f766c38980.json", scope)
client = gspread.authorize(creds)

GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"

# ëª¨ë¸ í† í° ì œí•œ
MAX_TOKENS = 8192
CHUNK_SIZE = 2000  # ë³´ë‚¼ ë•Œ ì—¬ìœ ë¥¼ ë‘ê¸° ìœ„í•œ í¬ê¸°

class LlmService:
    @staticmethod
    def extract_keywords_gpt(text: str, cm: str) -> list[str]:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        #OpenAI.api_key = os.getenv("OPENAI_API_KEY")

        prompt = f'''
                    #ëª…ë ¹ë¬¸
                    ë„ˆëŠ” ì£¼ì‹ ì• ë„ë¦¬ìŠ¤íŠ¸ì•¼.
                    ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” ë¯¸êµ­ ì£¼ì‹ì— ëŒ€í•´ ì–˜ê¸°í•˜ëŠ” { 'í•œêµ­' if cm == 'dc' else 'ì „ì„¸ê³„ì¸ì˜' } ì£¼ì‹ ì»¤ë®¤ë‹ˆí‹° ê²Œì‹œê¸€ë“¤ì„ ëª¨ì€ ê±°ì•¼.
                    ì´ í…ìŠ¤íŠ¸ë¥¼ ë³´ê³  ìœ ì €ë“¤ì´ í˜„ì¬ ê´€ì‹¬ ìˆëŠ” ì£¼ì‹ ì¢…ëª©ëª…(ticker)ê³¼ ì–´ë–¤ ê°ì •ì„ ê°–ê³  ìˆëŠ”ì§€ í‘œí˜„í•´ì¤˜. (ê¸ì •, ë¶€ì •, ì¤‘ë¦½)
                    ê·¸ ê°ì •ì„ ê°–ê³  ìˆëŠ” ì›ì¸ì„ ì•Œ ìˆ˜ ìˆë‹¤ë©´ ê°™ì´ ì¶œë ¥í•´ì¤˜.              

                    #ì˜ˆì‹œ
                    ğŸ“Œ [TSLA] (ê¸ì •): 4ì›” 27ì¼ ì‹¤ì  ë°œí‘œë¥¼ ì•ë‘ê³  ìˆìŒ. ì´ë¡œ ì¸í•œ ì£¼ê°€ ìƒìŠ¹ ê¸°ëŒ€ì¤‘.
                    ğŸ“Œ [NIKE] (ì¤‘ë¦½): ì¼ë¶€ëŠ” ê³µë§¤ë„ í¬ì§€ì…˜ì„ ì¡ê³  ì‹œì¥ì„ ì˜ˆìƒí•˜ëŠ” ì¤‘. ë‹¨ê¸° ë³€ë™ì„±ì— ëŒ€ë¹„í•˜ëŠ” ëª¨ìŠµ.
                    (ìƒëµ)
                '''
        
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": text}
            ]
        )

        # print(response.output_text)
        return response.choices[0].message.content

    # DC ìŠ¤í¬ë˜í•‘ ì €ì¥ëœ ê²Œì‹œê¸€ ì „ì²´ ì¡°íšŒ -> llmìœ¼ë¡œ ìš”ì•½ í›„ ì‹œíŠ¸ì— ì €ì¥
    @staticmethod
    async def summarize_by_llm_dc():
        start = time.time()

        # ìŠ¤í”„ë ˆë“œ ì‹œíŠ¸ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        sheet = client.open("stockus-posts").worksheet("posts")
        all_data = sheet.get_all_records()

        full_text = ''

        for post in all_data:
            full_text += post["title"] + " " + post["contents"]

        print(len(full_text)) # ì „ì²´ ê¸€ììˆ˜ í™•ì¸
            
        result = LlmService.extract_keywords_gpt(full_text, 'dc')
        #result = extract_keywords(" ".join(keywords_list))

        sheet = client.open("stockus-posts").worksheet("summary")

        KST = timezone(timedelta(hours=9))
        kst_now = datetime.now(KST)

        time_stamp = kst_now.strftime("%Y-%m-%d %H:%M:%S") # kst ê¸°ì¤€
        sheet.append_rows([[result, time_stamp]])
        
        end = time.time()

        return {"data": result, "time": f"{end - start: 0.2f}ì´ˆ"}

    # llm ìš”ì•½ ê¸€ ì¡°íšŒ
    # DC: ì‹œíŠ¸ì— ì €ì¥ëœ ìš”ì•½ ì¡°íšŒ / Reddit: ì‹œíŠ¸ ì €ì¥ x -> Reddit API í˜¸ì¶œ í›„ ë°”ë¡œ llm ìš”ì•½
    @staticmethod
    def llm_summary(cm: str):
        if cm == 'dc': # dc
            sheet = client.open("stockus-posts").worksheet("summary")
            all_values = sheet.get_all_values()
            last_row = all_values[-1] if all_values else None; 
        #print(last_row)
            
            return { 
                "text": last_row[0],
                "time_stamp": last_row[1]
            }
        elif cm == 'rd': # reddit

            text = json.dumps(RedditService.get_reddit_posts())
            result = LlmService.extract_keywords_gpt(text, 'rd')
            KST = timezone(timedelta(hours=9))
            kst_now = datetime.now(KST)

            return {
                "text": result,
                "time_stamp": kst_now.strftime("%Y-%m-%d %H:%M:%S") # kst ê¸°ì¤€
            }
        else: 
            return {
                "text": "ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤.",
                "time_stamp": ""
            }

    def extract_keywords_llama(text: str, count: int = 5) -> list[str]:
        # prompt = f'''ë‹¤ìŒì€ í•œêµ­ì¸ë“¤ì˜ ë¯¸êµ­ ì£¼ì‹ ì»¤ë®¤ë‹ˆí‹°ì˜ ê¸€ ë‚´ìš©ë“¤ì´ì•¼. ì‚¬ëŒë“¤ì´ ë¯¸êµ­ ì£¼ì‹ í˜„í™©ì— ëŒ€í•´ì„œ ì–´ë–»ê²Œ ìƒê°í•˜ê³  ìˆëŠ”ì§€ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜:\n{text}'''

        # prompt = f'''ë‹¤ìŒì€ ì£¼ì‹ ì»¤ë®¤ë‹ˆí‹°ì— ì˜¬ë¼ì˜¨ ê¸€ë“¤ì´ì•¼.\n
        #             ê° ê¸€ì—ì„œ ì‚¬ëŒë“¤ì´ ì–´ë–¤ ì¢…ëª©ì— ê´€ì‹¬ì„ ë³´ì´ê³  ìˆëŠ”ì§€, ê·¸ë¦¬ê³  ê·¸ ê°ì •ì´ ê¸ì •/ë¶€ì •/í˜¼í•© ì¤‘ ì–´ë–¤ì§€ ìš”ì•½í•´ì„œ ì•Œë ¤ì¤˜:\n
        #             {text}\n
        #             ìš”ì•½ ê²°ê³¼ëŠ” ì•„ë˜ì²˜ëŸ¼ ì •ë¦¬í•´ì¤˜:
        #             - í…ŒìŠ¬ë¼: ê¸ì • (ìƒìŠ¹ ê¸°ëŒ€ê°)
        #             - ì—”ë¹„ë””ì•„: ë¶€ì • (ê³¼ë§¤ìˆ˜ ìš°ë ¤)'''


        # prompt = f'''Here are some posts from a stock market community.\n
        #             Please summarize which stocks people are showing interest in and what kind of sentiment they are expressing â€” positive, negative, or mixed.\n
        #             Please organize the summary in the format below:
        #             {text}\n
        #             Tesla: Positive (Expecting price increase)\n
        #             Nvidia: Negative (Concerns of being overbought)'''
        
        prompt = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a financial data analyst summarizing posts from a Korean stock community. 
                    For each post, extract:
                    1. The company or stock mentioned (e.g. Tesla, Nvidia, Palantir)
                    2. The sentiment expressed by the user: Positive, Negative, or Mixed
                    3. The reason or context in 1 short sentence

                    Format your answer like this:
                    - Tesla: Positive (Expecting stock price to rise after strong sales report)
                    - Nvidia: Mixed (Concern about US export restrictions, but long-term growth expected)

                    Here is the posts: <|eot_id|><|start_header_id|>{text}<|end_header_id|>
                    
                    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>'''

        headers = {
            "Authorization": f"Bearer {GROQ_API_KEY}",
            "Content-Type": "application/json",
        }

        data = {
            "model": "llama3-70b-8192",
            "messages": [
                {"role": "user", "content": prompt}
            ]
        }

        chunks = [text[i:i+CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE)]
        print('chunks', len(chunks))
        results = []
        for i, chunk in enumerate(chunks):
            if i == 3:
                break
            print('chunk ê¸¸ì´ í™•ì¸', len(chunk))
            print('ëª‡ ë²ˆì§¸ chunk:', i)
            data["messages"][0]["content"] = chunk
            response = requests.post(GROQ_API_URL, headers=headers, json=data)
            response.raise_for_status()  # ìš”ì²­ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
            print(response.json())
            results.append(response.json()["choices"][0]["message"]["content"])
            time.sleep(random.uniform(2, 3))
        combined_text = " ".join(results)

        # response = requests.post(GROQ_API_URL, headers=headers, json=data)
        # response.raise_for_status()
        # result = response.json()

        # return result["choices"][0]["message"]["content"].strip().split("\n"

        # ìµœì¢… ìš”ì•½ ìš”ì²­
        # final_prompt = f'''ì§€ê¸ˆê¹Œì§€ ìš”ì•½í•œ í…ìŠ¤íŠ¸ë¥¼ ë§ˆì§€ë§‰ìœ¼ë¡œ í•œë²ˆ ë” ìš”ì•½í•˜ê³  í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì„œ 2~3ë¬¸ì¥ìœ¼ë¡œ ë§í•´ì¤˜:\n{combined_text}'''
        # data["messages"][0]["content"] = final_prompt

        # ìš”ì•½ ìš”ì²­
        # time.sleep(random.uniform(2, 3))

        # response = requests.post(GROQ_API_URL, headers=headers, json=data)
        # response.raise_for_status()  # ìš”ì²­ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
        # final_summary = response.json()["choices"][0]["message"]["content"]

        #return final_summary 
        return combined_text
    
    # í•œê¸€ ì „ì²˜ë¦¬ í•¨ìˆ˜
    def tokenize_korean(text):
        okt = Okt()
        return [word for word, pos in okt.pos(text) if pos in ["Noun", "Verb", "Adjective"]]

    # TF-IDFë¡œ í‚¤ì›Œë“œ ë½‘ê¸°
    def tfIdf():    

        # ìŠ¤í”„ë ˆë“œ ì‹œíŠ¸ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        sheet = client.open("stockus-posts").sheet1
        
        title_data = sheet.col_values(2)[1:]
        print(title_data)
        contents_data = sheet.col_values(6)[1:]
        print(contents_data)

        concat_list = [val for pair in zip(title_data, contents_data) for val in pair if val != '' or val != '- dc official App']   
        print(concat_list)
        full_text = " ".join(concat_list).replace("- dc official App","") # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹¨

        custom_stopwords = [
        # ì¶”ì„ìƒˆ, ë°˜ì‘
        'ã…‹ã…‹', 'ã…ã…', 'ã… ã… ', 'ã…œã…œ', 'ã„·ã„·', 'í—', 'ìŒ', 'ì™€', 'ì•„', 'ì˜¤', 'ìš”', 'ë„¤', 'ì‘', 'ì§„ì§œ', 'ê·¸ëƒ¥',

        # ë¶ˆí•„ìš”í•œ ë§¥ë½ ë‹¨ì–´
        'ì£¼ì‹', 'ì¢…ëª©', 'ì‹œì¥', 'ë‰´ìŠ¤', 'ì´ìŠˆ', 'ê¸€', 'ëŒ“ê¸€', 'ì˜ìƒ', 'ê¸°ì‚¬', 'ì •ë³´', 'ë¶„ì„',
        'íˆ¬ì', 'ë§¤ìˆ˜', 'ë§¤ë„', 'ë§¤ë§¤', 'ê°€ê²©', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì´ë²ˆ', 'ë‹¤ìŒ', 'ìµœê·¼', 'ì§€ê¸ˆ', 'ì•„ì§',

        # í‘œí˜„ + ì¡°ì‚¬
        'ê·¼ë°', 'ê·¸ëŸ°ë°', 'ë­”ê°€', 'ë­ì§€', 'ë­ì•¼', 'ë•Œë¬¸ì—', 'ê·¸ë¦¬ê³ ', 'ê·¸ë˜ì„œ', 'í•˜ì§€ë§Œ',
        'ì§„ì§œ', 'ë„ˆë¬´', 'ë§ì´', 'ì¢€', 'ì¢€ë”', 'ë”', 'ë˜ê²Œ', 'ë§ì´', 'ë§ìŒ', 'ë§ë‹¤',
        
        # ê¸°íƒ€ filler words
        'ì‚¬ëŒ', 'ê°œë¯¸', 'ì™¸ì¸', 'ê¸°ê´€', 'ë‚˜', 'ë„ˆ', 'ê±”', 'ìš°ë¦¬', 'ì´ê±°', 'ì €ê±°', 'ê·¸ê±°',
        
        # ìì£¼ ë‚˜ì˜¤ëŠ” ì•½ì–´/ë¬´ì˜ë¯¸ ë‹¨ì–´
        'ã…‡ã…‡', 'ã„´ã„´', 'ã……ã…‚', 'ã…ˆã„´', 'ã„¹ã…‡', 'ã…ã…Š', 'ã…‡ã…‹', 'ã„±ã„±', 'ã„´ã…‡ã……', 'ã…‚ã…‚',
        
        # ìˆ«ì/ë‹¨ìœ„
        'ì–µ', 'ë§Œì›', 'ì›', 'í¼ì„¼íŠ¸', 'í”„ë¡œ', 'ë‹¬ëŸ¬',

        # ì¢…ëª©ëª…ì— ìì£¼ ë¶™ëŠ” ë‹¨ì–´
        'ì§€ì£¼', 'í™€ë”©ìŠ¤', 'í…Œí¬', 'ë°”ì´ì˜¤', 'ë©', 'ì‚°ì—…', 'ì „ì', 'ì¸í„°ë‚´ì…”ë„', 'ê·¸ë£¹'
        ]

        # TF-IDF ë²¡í„°ë¼ì´ì €
        vectorizer = TfidfVectorizer(tokenizer=LlmService.tokenize_korean) # ì¼ë‹¨ ë¶ˆìš©ì–´ ì œì™¸

        # ë¶„ì„
        tfidf_matrix = vectorizer.fit_transform(concat_list)
        terms = vectorizer.get_feature_names_out()

        # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        scores = tfidf_matrix.toarray().sum(axis=0)
        #print(scores)
        keywords = sorted(zip(terms, scores), key=lambda x: x[1], reverse=True)
        #print(keywords)

        keywords_list = []

        # ì¶œë ¥ (ìƒìœ„ 10ê°œ)
        for word, score in keywords:
            # print(f"{word}: {round(score, 4)}")
            # print(word)
            keywords_list.append(word)
            print('keywords_list', keywords_list)

        return keywords_list